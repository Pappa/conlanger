{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"./data/language_phonemes.npz\", allow_pickle=True)\n",
    "language_phonemes_all = data[\"language_phonemes_all\"]\n",
    "language_names_all = data[\"language_names_all\"]\n",
    "language_phonemes_selected = data[\"language_phonemes_selected\"]\n",
    "language_names_selected = data[\"language_names_selected\"]\n",
    "\n",
    "assert language_phonemes_all.shape[0] == language_names_all.shape[0]\n",
    "assert language_phonemes_selected.shape[0] == language_names_selected.shape[0]\n",
    "language_phonemes_all.shape, language_names_all.shape, language_phonemes_selected.shape, language_names_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names, unique_names_count = np.unique(language_names_all, return_counts=True)\n",
    "multiple_samples = unique_names[unique_names_count > 1]\n",
    "\n",
    "count_sort_idx_desc = np.argsort(-unique_names_count)\n",
    "\n",
    "most_common_languages = list(\n",
    "    zip(unique_names[count_sort_idx_desc], unique_names_count[count_sort_idx_desc])\n",
    ")\n",
    "\n",
    "print(most_common_languages[0:5])\n",
    "\n",
    "X = language_phonemes_all[np.in1d(language_names_all, multiple_samples)]\n",
    "y = language_names_all[np.in1d(language_names_all, multiple_samples)]\n",
    "\n",
    "sample_shape = X[0].shape\n",
    "num_classes = len(unique_names)\n",
    "\n",
    "y.shape, X.shape, sample_shape, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.39, random_state=33)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[0], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Sequential, layers\n",
    "\n",
    "# Create network architecture\n",
    "\n",
    "input_shape = X[0].shape\n",
    "\n",
    "print(f\"shape={input_shape}\")\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ],\n",
    "    name=\"language_model\"\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compilation\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy doesn't look great, but it's not bad considering there\n",
    "# is only one training sample of many of the languages and the\n",
    "# number of classes is in the thousands\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Classes: {num_classes}\")\n",
    "print(f\"Samples per language: {num_classes / len(y_train):.2f}\")\n",
    "print(f\"Random guess probability: {1 / num_classes:.5f}\")\n",
    "print(\n",
    "    f\"Most common language probability: {most_common_languages[0][1] / num_classes:.5f}\"\n",
    ")\n",
    "print(\n",
    "    f\"5 most common language probability: {np.sum([y for _, y in most_common_languages[0:5]]) / num_classes:.5f}\"\n",
    ")\n",
    "print(f\"Model accuracy: {test_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictions are heavily skewed towards languages that appear multiple\n",
    "# times in the dataset (to be expected really). So, to genrate synthetic examples\n",
    "# that don't just copy the most common languages, it'll probably be necessary to \n",
    "# limit the trainling data for synthetic data genertion to 1 example or a small \n",
    "# number of examples per language.\n",
    "\n",
    "num_samples = 50\n",
    "top_n_languages = set([name for name, _ in most_common_languages[0:5]])\n",
    "\n",
    "predictions = label_encoder.inverse_transform(\n",
    "    model.predict(X_test[0:num_samples]).argmax(axis=-1)\n",
    ")\n",
    "actual = label_encoder.inverse_transform(y_test[0:num_samples].argmax(axis=-1))\n",
    "\n",
    "correct_predictions = sum([1 if p == a else 0 for p, a in zip(predictions, actual)])\n",
    "\n",
    "predicted_from_common = sum([1 if p in top_n_languages else 0 for p in predictions])\n",
    "\n",
    "print(f\"Correct preditions: {correct_predictions / len(predictions) * 100}%\")\n",
    "print(f\"Predicted from top 5: {predicted_from_common / len(predictions) * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
